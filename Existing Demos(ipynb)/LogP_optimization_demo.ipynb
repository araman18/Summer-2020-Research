{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogP optimization with ReLeaSE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will optimized parameters of pretrained generative RNN to produce molecules with values of logP within drug-like region according to Lipinsky rule. We use policy gradient algorithm with custom reward function to bias the properties of generated molecules aka Reinforcement Learninf for Structural Evolution (ReLeaSE) as was proposed in **Popova, M., Isayev, O., & Tropsha, A. (2018). *Deep reinforcement learning for de novo drug design*. Science advances, 4(7), eaap7885.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./release/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "from rdkit import Chem, DataStructs\n",
    "from stackRNN import StackAugmentedRNN\n",
    "from data import GeneratorData \n",
    "from utils import canonical_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_path = './data/chembl_22_clean_1576904_sorted_std_final.smi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<', '>', '#', '%', ')', '(', '+', '-', '/', '.', '1', '0', '3', '2', '5', '4', '7',\n",
    "          '6', '9', '8', '=', 'A', '@', 'C', 'B', 'F', 'I', 'H', 'O', 'N', 'P', 'S', '[', ']',\n",
    "          '\\\\', 'c', 'e', 'i', 'l', 'o', 'n', 'p', 's', 'r', '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['CCO\\tCHEMBL545']) list(['C\\tCHEMBL17564'])\n",
      " list(['CO\\tCHEMBL14688']) ...\n",
      " list(['n1(cnc2c1N=C(N)NC2=O)C1OC(COP(O)(=O)OC2C(COP(O)(=O)OC3CC(OC3COP(O)(=O)OC3CC(OC3COP(O)(=O)OC3C(COP(O)(=O)OC4C(COP(O)(=O)OC5C(COP(O)(=O)OC6C(COP(O)(=O)OC7C(COP(O)(=O)OC8C(COP(O)(=O)OC9C(COP(O)(=O)OC%10C(COP(O)(=O)OC%11CC(OC%11COP(O)(=O)OC%11C(COP(O)(=O)OC%12C(COP(O)(=O)OC%13CC(OC%13COP(O)(=O)OC%13C(COP(O)(=O)OC%14C(COP(O)(=O)OC%15C(COP(O)(=O)OC%16CC(OC%16COP(O)(=O)OC%16C(COP(O)(=O)OC%17C(COc%18ccc%19C%20(OC(=O)c%21ccccc%20%21)c%20ccc(O)cc%20Oc%19c%18)OC(n%18c%19c(nc%18)c(N)ncn%19)C%17O)OC(C%16O)n%16cnc%17c%16N=C(N)NC%17=O)N%16C=C(C)C(=O)NC%16=O)OC(C%15O)N%15C=CC(N)=NC%15=O)OC(C%14O)N%14C=CC(N)=NC%14=O)OC(C%13O)n%13cnc%14c%13N=C(N)NC%14=O)N%13C=C(C)C(=O)NC%13=O)OC(C%12O)n%12cnc%13c%12N=C(N)NC%13=O)OC(C%11O)n%11cnc%12c%11N=C(N)NC%12=O)N%11C=C(C)C(=O)NC%11=O)OC(n%11c%12c(nc%11)c(N)ncn%12)C%10O)OC(n%10cnc%11c%10N=C(N)NC%11=O)C9O)OC(n9cnc%10c9N=C(N)NC%10=O)C8O)OC(C7O)n7cnc8c7N=C(N)NC8=O)OC(C6O)N6C=CC(N)=NC6=O)OC(n6c7c(nc6)c(N)ncn7)C5O)OC(n5cnc6c5N=C(N)NC6=O)C4O)OC(C3O)n3cnc4c3N=C(N)NC4=O)N3C=C(C)C(=O)NC3=O)N3C=C(C)C(=O)NC3=O)OC(C2O)n2cnc3c2N=C(N)NC3=O)C(OP(O)(=O)OCC2OC(C(O)C2OP(O)(=O)OCC2OC(C(O)C2OP(O)(=O)OCC2OC(CC2OP(O)(=O)OCC2OC(C(O)C2OP(O)(=O)OCC2OC(n3c4c(nc3)c(N)ncn4)C(O)C2OP(O)(=O)OCC2OC(C(O)C2OP(O)(=O)OCC2OC(CC2OP(O)(=O)OCC23OC(C(OCOC2c2ccccc2)C3O)N2C=C(C)C(=O)NC2=O)N2C=C(C)C(=O)NC2=O)N2C=CC(N)=NC2=O)n2cnc3c2NC(N)=NC3=O)N2C=C(C)C(=O)NC2=C)n2cnc3c2NC(N)=NC3=O)n2cnc3c2N=C(N)NC3=O)C1O\\tCHEMBL1077165'])\n",
      " list(['CC1=CN(C2CC(OP(O)(=O)OCC3OC(C(O)C3OP(O)(=O)OCC3OC(C(O)C3OP(O)(=O)OCC3OC(C(O)C3OP(O)(=O)OCC3OC(CC3OP(O)(=O)OCC34CNOC(C3O)C(O4)N3C=C(C)C(=O)NC3=O)N3C=C(C)C(=O)NC3=O)N3C=CC(N)=NC3=O)n3cnc4c(N)ncnc34)n3cnc4c3NC(N)=NC4=O)C(COP(O)(=O)OC3C(COP(O)(=O)OC4C(COP(O)(=O)OC5C(COP(O)(=O)OC6C(COP(O)(=O)OC7CC(OC7COP(O)(=O)OC7CC(OC7COP(O)(=O)OC7C(COP(O)(=O)OC8C(COP(O)(=O)OC9C(COP(O)(=O)OC%10C(COP(O)(=O)OC%11C(COP(O)(=O)OC%12C(COP(O)(=O)OC%13C(COP(O)(=O)OC%14C(COP(O)(=O)OC%15CC(OC%15COP(O)(=O)OC%15C(COP(O)(=O)OC%16C(COP(O)(=O)OC%17CC(OC%17COP(O)(=O)OC%17C(COP(O)(=O)OC%18C(COP(O)(=O)OC%19C(COP(O)(=O)OC%20CC(OC%20COP(O)(=O)OC%20C(COP(O)(=O)OC%21C(COc%22ccc%23c(Oc%24cc(O)ccc%24C%23%23OC(=O)c%24ccccc%23%24)c%22)OC(C%21O)n%21cnc%22c(N)ncnc%21%22)OC(C%20O)n%20cnc%21c%20N=C(N)NC%21=O)N%20C=C(C)C(=O)NC%20=O)OC(C%19O)N%19C=CC(N)=NC%19=O)OC(C%18O)N%18C=CC(N)=NC%18=O)OC(C%17O)n%17cnc%18c%17N=C(N)NC%18=O)N%17C=C(C)C(=O)NC%17=O)OC(C%16O)n%16cnc%17c%16N=C(N)NC%17=O)OC(C%15O)n%15cnc%16c%15N=C(N)NC%16=O)N%15C=C(C)C(=O)NC%15=O)OC(C%14O)n%14cnc%15c(N)ncnc%14%15)OC(C%13O)n%13cnc%14c%13N=C(N)NC%14=O)OC(C%12O)n%12cnc%13c%12N=C(N)NC%13=O)OC(C%11O)n%11cnc%12c%11N=C(N)NC%12=O)OC(C%10O)N%10C=CC(N)=NC%10=O)OC(C9O)n9cnc%10c(N)ncnc9%10)OC(C8O)n8cnc9c8N=C(N)NC9=O)OC(C7O)n7cnc8c7N=C(N)NC8=O)N7C=C(C)C(=O)NC7=O)N7C=C(C)C(=O)NC7=O)OC(C6O)n6cnc7c6N=C(N)NC7=O)OC(C5O)n5cnc6c5N=C(N)NC6=O)OC(C4O)n4cnc5c4N=C(N)NC5=O)OC(C3O)n3cnc4c3NC(N)=NC4=O)O2)C(=C)NC1=O\\tCHEMBL1077164'])\n",
      " list(['CC1=CN(C2CC(OP(O)(=O)OCC3OC(C(O)C3OP(O)(=O)OCC3OC(C(O)C3OP(O)(=O)OCC3OC(C(O)C3OP(O)(=O)OCC3OC(CC3OP(O)(=O)OCC34COCOC(C3O)C(O4)N3C=C(C)C(=O)NC3=O)N3C=C(C)C(=O)NC3=O)N3C=CC(N)=NC3=O)n3cnc4c(N)ncnc34)n3cnc4c3NC(N)=NC4=O)C(COP(O)(=O)OC3C(COP(O)(=O)OC4C(COP(O)(=O)OC5C(COP(O)(=O)OC6C(COP(O)(=O)OC7CC(OC7COP(O)(=O)OC7CC(OC7COP(O)(=O)OC7C(COP(O)(=O)OC8C(COP(O)(=O)OC9C(COP(O)(=O)OC%10C(COP(O)(=O)OC%11C(COP(O)(=O)OC%12C(COP(O)(=O)OC%13C(COP(O)(=O)OC%14C(COP(O)(=O)OC%15CC(OC%15COP(O)(=O)OC%15C(COP(O)(=O)OC%16C(COP(O)(=O)OC%17CC(OC%17COP(O)(=O)OC%17C(COP(O)(=O)OC%18C(COP(O)(=O)OC%19C(COP(O)(=O)OC%20CC(OC%20COP(O)(=O)OC%20C(COP(O)(=O)OC%21C(COc%22ccc%23c(Oc%24cc(O)ccc%24C%23%23OC(=O)c%24ccccc%23%24)c%22)OC(C%21O)n%21cnc%22c(N)ncnc%21%22)OC(C%20O)n%20cnc%21c%20N=C(N)NC%21=O)N%20C=C(C)C(=O)NC%20=O)OC(C%19O)N%19C=CC(N)=NC%19=O)OC(C%18O)N%18C=CC(N)=NC%18=O)OC(C%17O)n%17cnc%18c%17N=C(N)NC%18=O)N%17C=C(C)C(=O)NC%17=O)OC(C%16O)n%16cnc%17c%16N=C(N)NC%17=O)OC(C%15O)n%15cnc%16c%15N=C(N)NC%16=O)N%15C=C(C)C(=O)NC%15=O)OC(C%14O)n%14cnc%15c(N)ncnc%14%15)OC(C%13O)n%13cnc%14c%13N=C(N)NC%14=O)OC(C%12O)n%12cnc%13c%12N=C(N)NC%13=O)OC(C%11O)n%11cnc%12c%11N=C(N)NC%12=O)OC(C%10O)N%10C=CC(N)=NC%10=O)OC(C9O)n9cnc%10c(N)ncnc9%10)OC(C8O)n8cnc9c8N=C(N)NC9=O)OC(C7O)n7cnc8c7N=C(N)NC8=O)N7C=C(C)C(=O)NC7=O)N7C=C(C)C(=O)NC7=O)OC(C6O)n6cnc7c6N=C(N)NC7=O)OC(C5O)n5cnc6c5N=C(N)NC6=O)OC(C4O)n4cnc5c4N=C(N)NC5=O)OC(C3O)n3cnc4c3NC(N)=NC4=O)O2)C(=C)NC1=O\\tCHEMBL1077163'])]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5e3b11453f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m gen_data = GeneratorData(training_data_path=gen_data_path, delimiter='\\t', \n\u001b[0;32m----> 2\u001b[0;31m                          cols_to_read=[0], keep_header=True, tokens=tokens)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ReLeaSE/release/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data_path, tokens, start_token, end_token, max_len, use_cuda, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         data = read_object_property_file(training_data_path,\n\u001b[0;32m---> 58\u001b[0;31m                                                        **kwargs)\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ReLeaSE/release/utils.py\u001b[0m in \u001b[0;36mread_object_property_file\u001b[0;34m(path, delimiter, cols_to_read, keep_header)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols_to_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols_to_read\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_position\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols_to_read\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "gen_data = GeneratorData(training_data_path=gen_data_path, delimiter='\\t', \n",
    "                         cols_to_read=[0], keep_header=True, tokens=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot_hist** function plots histogram of predicted properties and a vertical line for thershold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(prediction, n_to_generate):\n",
    "    prediction = np.array(prediction)\n",
    "    percentage_in_threshold = np.sum((prediction >= 0.0) & \n",
    "                                     (prediction <= 5.0))/len(prediction)\n",
    "    print(\"Percentage of predictions within drug-like region:\", percentage_in_threshold)\n",
    "    print(\"Proportion of valid SMILES:\", len(prediction)/n_to_generate)\n",
    "    ax = sns.kdeplot(prediction, shade=True)\n",
    "    plt.axvline(x=0.0)\n",
    "    plt.axvline(x=5.0)\n",
    "    ax.set(xlabel='Predicted LogP', \n",
    "           title='Distribution of predicted LogP for generated molecules')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**estimate_and_update** function:\n",
    "\n",
    "1) generates n_to_generate number of SMILES strings\n",
    "\n",
    "2) filters invalid SMILES\n",
    "\n",
    "3) predicts logP for valid SMILES\n",
    "\n",
    "4) plots histogram of predicted logP\n",
    "\n",
    "5) Returns valid SMILES and their predicted logPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_and_update(generator, predictor, n_to_generate):\n",
    "    generated = []\n",
    "    pbar = tqdm(range(n_to_generate))\n",
    "    for i in pbar:\n",
    "        pbar.set_description(\"Generating molecules...\")\n",
    "        generated.append(generator.evaluate(gen_data, predict_len=120)[1:-1])\n",
    "\n",
    "    sanitized = canonical_smiles(generated, sanitize=False, throw_warning=False)[:-1]\n",
    "    unique_smiles = list(np.unique(sanitized))[1:]\n",
    "    smiles, prediction, nan_smiles = predictor.predict(unique_smiles, use_tqdm=True)  \n",
    "                                                       \n",
    "    plot_hist(prediction, n_to_generate)\n",
    "        \n",
    "    return smiles, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and training the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will used stack augmented generative GRU as a generator. The model was trained to predict the next symbol from SMILES alphabet using the already generated prefix. Model was trained to minimize the cross-entropy loss between predicted symbol and ground truth symbol. Scheme of the generator when inferring new SMILES is shown below:\n",
    "\n",
    "<img src=\"./figures/generator.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize stack-augmented generative RNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want train the model from scratch, uncomment the lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './checkpoints/generator/checkpoint_biggest_rnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#losses = my_generator.fit(gen_data, 1500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_generator.evaluate(gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_generator.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can skip the process of training and load the pretrained parameters into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1500\n",
    "stack_width = 1500\n",
    "stack_depth = 200\n",
    "layer_type = 'GRU'\n",
    "lr = 0.001\n",
    "optimizer_instance = torch.optim.Adadelta\n",
    "\n",
    "my_generator = StackAugmentedRNN(input_size=45, hidden_size=hidden_size,\n",
    "                                 output_size=45, layer_type=layer_type,\n",
    "                                 n_layers=1, is_bidirectional=False, has_stack=True,\n",
    "                                 stack_width=stack_width, stack_depth=stack_depth, \n",
    "                                 use_cuda=False, \n",
    "                                 optimizer_instance=optimizer_instance, lr=lr)\n",
    "my_generator.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo we will use Recurrent Neural Network, i.e. unidirectional LSTM with 2 layers. The network is trained in 5-fold cross validation manner using the OpenChem toolkit (https://github.com/Mariewelt/OpenChem). In this demo we only upload the pretrained model. The training demo is in *RecurrentQSAR-example-logp.ipynb* file in the same directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'OpenChem' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "! git clone --single-branch --branch develop https://github.com/Mariewelt/OpenChem.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./OpenChem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_predictor import RNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_tokens = tokens + [' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_params = './checkpoints/logP/model_parameters.pkl'\n",
    "path_to_checkpoint = './checkpoints/logP/fold_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictor = RNNPredictor(path_to_params, path_to_checkpoint, predictor_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we produce the unbiased distribution of the property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smiles_unbiased, prediction_unbiased = estimate_and_update(my_generator,\n",
    "                                                           my_predictor,\n",
    "                                                           n_to_generate=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biasing the distribution of the generator with reinforcement learning (policy gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the generator and the predictor into a single pipeline. The generator produces new SMILES string, which is then evaluated by the predictor. Based on the obtain prediction and our goal, we assign a numerical reward value and update the parameters of the generator using policy gradient algorithm.\n",
    "\n",
    "<img src=\"./figures/rl_pipeline.png\">\n",
    "\n",
    "Policy gradient loss is defined as:\n",
    "$$\n",
    "L(S|\\theta) = -\\dfrac{1}{n}\\sum_{i=1}^{|S|} \\sum_{j=1}^{length(s_i)} R_i\\cdot \\gamma^i \\cdot \\log p(s_i|s_0 \\dots s_{i-1}\\theta),\n",
    "$$\n",
    "\n",
    "where $R_i$ is the reward obtained at time step $i$ $\\gamma$ is the discount factor and $p(s_i|s_0 \\dots s_{i-1}, \\theta)$ is the probability of the next character given the prefix, which we obtain from the generator. \n",
    "\n",
    "In our case the reward is the same for every time step and is equal to the reward for the whole molecule. Discount factor $\\gamma$ is a number close to $1.0$ (it could be $1.0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing logP to be in drug like region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reinforcement'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1210a70fcee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreinforcement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReinforcement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reinforcement'"
     ]
    }
   ],
   "source": [
    "from reinforcement import Reinforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a copy of the generator that will be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_generator_max = StackAugmentedRNN(input_size=gen_data.n_characters, \n",
    "                                     hidden_size=hidden_size,\n",
    "                                     output_size=gen_data.n_characters, \n",
    "                                     layer_type=layer_type,\n",
    "                                     n_layers=1, is_bidirectional=False, has_stack=True,\n",
    "                                     stack_width=stack_width, stack_depth=stack_depth, \n",
    "                                     use_cuda=use_cuda, \n",
    "                                     optimizer_instance=optimizer_instance, lr=lr)\n",
    "\n",
    "my_generator_max.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up some parameters for the experiment\n",
    "n_to_generate = 200\n",
    "n_policy_replay = 10\n",
    "n_policy = 15\n",
    "n_iterations = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_moving_average(previous_values, new_value, ma_window_size=10):\n",
    "    value_ma = np.sum(previous_values[-(ma_window_size-1):]) + new_value\n",
    "    value_ma = value_ma/(len(previous_values[-(ma_window_size-1):]) + 1)\n",
    "    return value_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function we will use here is the following:\n",
    "\n",
    "$$\n",
    "R =  \\begin{cases} 11.0, & \\mbox{if } 1.0 < \\log P < 4.0 \\\\ 1.0, & \\mbox{otherwise}  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_logp(smiles, predictor, invalid_reward=0.0):\n",
    "    mol, prop, nan_smiles = predictor.predict([smiles])\n",
    "    if len(nan_smiles) == 1:\n",
    "        return invalid_reward\n",
    "    if (prop[0] >= 1.0) and (prop[0] <= 4.0):\n",
    "        return 11.0\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 12)\n",
    "reward = lambda x: 11.0 if ((x > 1.0) and (x < 4.0)) else 1.0\n",
    "plt.plot(x, [reward(i) for i in x])\n",
    "plt.xlabel('logP value')\n",
    "plt.ylabel('Reward value')\n",
    "plt.title('Reward function for logP optimization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_logp = Reinforcement(my_generator_max, my_predictor, get_reward_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rl_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_iterations):\n",
    "    for j in trange(n_policy, desc='Policy gradient...'):\n",
    "        cur_reward, cur_loss = RL_logp.policy_gradient(gen_data)\n",
    "        rewards.append(simple_moving_average(rewards, cur_reward)) \n",
    "        rl_losses.append(simple_moving_average(rl_losses, cur_loss))\n",
    "    \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.show()\n",
    "    plt.plot(rl_losses)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "        \n",
    "    smiles_cur, prediction_cur = estimate_and_update(RL_logp.generator, \n",
    "                                                     my_predictor, \n",
    "                                                     n_to_generate)\n",
    "    print('Sample trajectories:')\n",
    "    for sm in smiles_cur[:5]:\n",
    "        print(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smiles_biased, prediction_biased = estimate_and_update(RL_logp.generator, \n",
    "                                                       my_predictor,\n",
    "                                                       n_to_generate=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(prediction_biased, label='Optimized', shade=True, color='purple')\n",
    "sns.kdeplot(prediction_unbiased, label='Unbiased', shade=True, color='grey')\n",
    "plt.xlabel('Predicted logP values')\n",
    "plt.title('Initial and biased distributions of log P')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing random molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will draw some random compounds from the biased library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Draw import DrawingOptions\n",
    "from rdkit.Chem import Draw\n",
    "DrawingOptions.atomLabelFontSize = 50\n",
    "DrawingOptions.dotsPerAngstrom = 100\n",
    "DrawingOptions.bondLineWidth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smiles_biased' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b940797e2fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_mols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmiles_biased\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'smiles_biased' is not defined"
     ]
    }
   ],
   "source": [
    "generated_mols = [Chem.MolFromSmiles(sm, sanitize=True) for sm in smiles_biased]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_gen_mols = [generated_mols[i] for i in np.where(np.array(generated_mols) != None)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_draw = 20\n",
    "ind = np.random.randint(0, len(sanitized_gen_mols), n_to_draw)\n",
    "mols_to_draw = [sanitized_gen_mols[i] for i in ind]\n",
    "legends = ['log P = ' + str(prediction_biased[i]) for i in ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Draw.MolsToGridImage(mols_to_draw, molsPerRow=5, \n",
    "                     subImgSize=(200,200), legends=legends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
